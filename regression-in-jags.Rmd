---
title: "regression-in-jags"
author: "Andrew L Jackson"
date: "26/7/2018"
output: html_document
---

## Fitting a linear regression model using JAGS

Fitting a linear regression model is similarly straight forward.

We can set up some simple data to try it on.

```{r example-reg-data}

x <- c(18.07, 52.59, 54.93, 79.31, 89.58)

y <- c(7.89, 12.41, 13.34, 19.3, 19.52)

plot(y ~ x,
     xlab = 'Percentage of food source in diet (x)',
     ylab = 'Weight (y)',
     las = 1,
     xlim = c(0,100),
     ylim = c(0, max(y)*1.1), 
     bty = "L")

m1 <- glm(y ~ x)

summary(m1)

abline(m1, col='red')
```

Then, as before, we define our JAGS model and pass it the data
```{r}
library(rjags)

modelstring ='
model {

  # Define the Likelihood
  for(i in 1:N) { 
    y[i] ~ dnorm(alpha + beta*x[i], sigma^-2) 
  }

  # -------------------------------------------
  # Prior distributions on our three parameters
  # Note: vague priors
  alpha ~ dnorm(0, 100^-2) 
  beta  ~ dnorm(0, 100^-2)
  sigma ~ dunif(0, 100)
}
'

# bundle the data into a list for passing to rjags
data <- list(x = x,
             y = y,
             N = 5)

# set up the model
model <- jags.model(textConnection(modelstring), 
                    data= data, 
                    n.chains = 3)

# evaluate the model
output <- coda.samples(model = model,
                       variable.names = c("alpha", 
                                          "beta",
                                          "sigma"),
                       n.iter = 10000)
```

**TASKS:** 

- check the model output for consistency with a linear model fitting using glm() and satisfy yourself that the slope and intercept are comparable. 
- Check for convergence.
- Explore the correlation between the slope and intercept. Would you a priori expec them to be correlated in some way? The best way to do this is to plot the posterior draws for the slope against the intercept, and run a correlation test using `cor.test()` on the two vectors.
- We estimated the residual error term `sd` but we didnt monitor it. Edit the code so we can obtain the posterior samples for this parameter too.

A key goal of regression modelling is to determine whether our esimate of the slope differs from zero. In frequentist statistics this would entail calculating a p-value which would tell us how likely the observed slope is given a null model where there is no true relationship between the two variables. In Bayesian statistics we can ask more directly what is the probabiliy that our posterior estimate of the slope does or does not contain zero. In the first instance, we can simply calculate some credible intervals of the estimated slope, and look to see if these cross zero or not. If say the 95% credible intervals do not contain zero, then there is a greater than 95% probability that our slope is different from zero.

```{r summary-jags-output}

summary(output)

```
The reported quantiles for `beta` do not include zero, and so we can report that our slope differs from zero with probability greater than 0.95.

We can also calculate that our slope is greater (or less than) zero directly, and obtain an accompanying probability for this estimate.

```{r prob-greater-than-zero}

p_beta_gt_zero <- sum(output[[1]][,"beta"] > 0) / length(output[[1]][,"beta"])

print(p_beta_gt_zero)

```

## Task completion

```{r plot-cor}

# plot the correlation between alpha and beta 
# for chain 1

plot.default(output[[1]][, "alpha"], 
             output[[1]][, "beta"], 
             cex = 0.7, 
             col= 1, 
             pch = 19)

cor.test(output[[1]][, "alpha"], 
             output[[1]][, "beta"])




```


